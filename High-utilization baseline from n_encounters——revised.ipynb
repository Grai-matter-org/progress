{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77609cbc-119c-43ba-ace5-ded3464942f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /Users/KrisLiu/Downloads/patient_features_final.csv | shape=(16369, 69)\n",
      "Targets to evaluate: ['n_encounters', 'n_procedures', 'overall_stay_days_mean', 'overall_stay_days_max', 'total_transfers', 'n_distinct_event_types', 'rx_orders_per_encounter_avg', 'rx_admin_per_encounter_avg']\n",
      "\n",
      "=== Target: n_encounters | prev=20.698% | rule >= ceil(p80) with used threshold=34 ===\n",
      "LogReg         | AUROC 0.914±0.003 | AUPRC 0.789±0.012 | feats=54\n",
      "RandForest     | AUROC 0.858±0.001 | AUPRC 0.632±0.008 | feats=54\n",
      "ExtraTrees     | AUROC 0.857±0.007 | AUPRC 0.686±0.015 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.913±0.003 | AUPRC 0.789±0.012 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_n_encounters_LogReg_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_n_encounters_LogReg_strict.png\n",
      "\n",
      "=== Target: n_procedures | prev=20.637% | rule >= ceil(p80) with used threshold=35 ===\n",
      "LogReg         | AUROC 0.804±0.013 | AUPRC 0.545±0.032 | feats=54\n",
      "RandForest     | AUROC 0.791±0.012 | AUPRC 0.515±0.028 | feats=54\n",
      "ExtraTrees     | AUROC 0.762±0.007 | AUPRC 0.488±0.020 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.803±0.012 | AUPRC 0.542±0.031 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_n_procedures_LogReg_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_n_procedures_LogReg_strict.png\n",
      "\n",
      "=== Target: overall_stay_days_mean | prev=20.001% | rule >= p80 with used threshold=7.223055555555559 ===\n",
      "LogReg         | AUROC 0.616±0.014 | AUPRC 0.273±0.010 | feats=54\n",
      "RandForest     | AUROC 0.620±0.013 | AUPRC 0.273±0.014 | feats=54\n",
      "ExtraTrees     | AUROC 0.597±0.012 | AUPRC 0.265±0.007 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.616±0.012 | AUPRC 0.269±0.008 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_overall_stay_days_mean_RandForest_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_overall_stay_days_mean_RandForest_strict.png\n",
      "\n",
      "=== Target: overall_stay_days_max | prev=31.444% | rule >= p80 with used threshold=91.0 ===\n",
      "LogReg         | AUROC 0.729±0.009 | AUPRC 0.547±0.012 | feats=54\n",
      "RandForest     | AUROC 0.716±0.010 | AUPRC 0.517±0.010 | feats=54\n",
      "ExtraTrees     | AUROC 0.698±0.011 | AUPRC 0.508±0.013 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.729±0.009 | AUPRC 0.546±0.011 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_overall_stay_days_max_LogReg_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_overall_stay_days_max_LogReg_strict.png\n",
      "\n",
      "=== Target: total_transfers | prev=18.902% | rule >= p80 with used threshold=19.0 ===\n",
      "LogReg         | AUROC 0.821±0.011 | AUPRC 0.580±0.024 | feats=54\n",
      "RandForest     | AUROC 0.808±0.009 | AUPRC 0.531±0.022 | feats=54\n",
      "ExtraTrees     | AUROC 0.764±0.019 | AUPRC 0.492±0.024 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.822±0.011 | AUPRC 0.582±0.024 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_total_transfers_LinearSVM_cal_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_total_transfers_LinearSVM_cal_strict.png\n",
      "\n",
      "=== Target: n_distinct_event_types | prev=42.916% | rule >= ceil(p80) with used threshold=6 ===\n",
      "LogReg         | AUROC 0.794±0.004 | AUPRC 0.731±0.008 | feats=54\n",
      "RandForest     | AUROC 0.793±0.005 | AUPRC 0.727±0.005 | feats=54\n",
      "ExtraTrees     | AUROC 0.751±0.010 | AUPRC 0.683±0.015 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.792±0.004 | AUPRC 0.729±0.008 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_n_distinct_event_types_LogReg_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_n_distinct_event_types_LogReg_strict.png\n",
      "\n",
      "=== Target: rx_orders_per_encounter_avg | prev=20.105% | rule >= p80 with used threshold=2.1667 ===\n",
      "LogReg         | AUROC 0.785±0.006 | AUPRC 0.448±0.011 | feats=54\n",
      "RandForest     | AUROC 0.786±0.011 | AUPRC 0.489±0.014 | feats=54\n",
      "ExtraTrees     | AUROC 0.713±0.011 | AUPRC 0.371±0.012 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.783±0.006 | AUPRC 0.445±0.009 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_rx_orders_per_encounter_avg_RandForest_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_rx_orders_per_encounter_avg_RandForest_strict.png\n",
      "\n",
      "=== Target: rx_admin_per_encounter_avg | prev=20.001% | rule >= p80 with used threshold=8.297460000000012 ===\n",
      "LogReg         | AUROC 0.797±0.007 | AUPRC 0.475±0.005 | feats=54\n",
      "RandForest     | AUROC 0.805±0.013 | AUPRC 0.469±0.028 | feats=54\n",
      "ExtraTrees     | AUROC 0.715±0.016 | AUPRC 0.386±0.020 | feats=54\n",
      "LinearSVM_cal  | AUROC 0.795±0.007 | AUPRC 0.473±0.005 | feats=54\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/roc_rx_admin_per_encounter_avg_RandForest_strict.png\n",
      "Saved: /Users/KrisLiu/Downloads/multi_target_eval/figures/pr_rx_admin_per_encounter_avg_RandForest_strict.png\n",
      "\n",
      "Wrote mega table: /Users/KrisLiu/Downloads/multi_target_eval/tables/mega_auc_table_strict.csv\n"
     ]
    }
   ],
   "source": [
    "# ================= Multi-target, multi-model AUC benchmarking (entry-only features)\n",
    "import os, re, inspect, warnings  # stdlib\n",
    "import numpy as np, pandas as pd  # core\n",
    "import matplotlib.pyplot as plt  # plots\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold  # CV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve  # metrics\n",
    "from sklearn.pipeline import Pipeline  # pipeline\n",
    "from sklearn.compose import ColumnTransformer  # column-wise transforms\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler  # enc/scale\n",
    "from sklearn.impute import SimpleImputer  # missing\n",
    "from sklearn.linear_model import LogisticRegression  # LR\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier  # RF/ET\n",
    "from sklearn.svm import LinearSVC  # SVM base\n",
    "from sklearn.calibration import CalibratedClassifierCV  # probability for SVM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # keep logs clean\n",
    "\n",
    "# ---------- Paths ----------\n",
    "DATA_PATH = os.path.expanduser(\"/Users/KrisLiu/Downloads/patient_features_final.csv\")  # input csv\n",
    "OUT_DIR   = os.path.expanduser(\"/Users/KrisLiu/Downloads/multi_target_eval\")  # outputs\n",
    "FIG_DIR   = os.path.join(OUT_DIR, \"figures\")  # fig dir\n",
    "TAB_DIR   = os.path.join(OUT_DIR, \"tables\")  # table dir\n",
    "FLIST_DIR = os.path.join(OUT_DIR, \"feature_lists\")  # feature lists dir\n",
    "for d in [OUT_DIR, FIG_DIR, TAB_DIR, FLIST_DIR]: os.makedirs(d, exist_ok=True)  # ensure dirs\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"  # guard\n",
    "df = pd.read_csv(DATA_PATH)  # load data\n",
    "print(f\"Loaded: {DATA_PATH} | shape={df.shape}\")  # status\n",
    "\n",
    "# ---------- Candidate targets (meters) ----------\n",
    "# We will try these meters if they exist; threshold = 0.80 by default.                                   \n",
    "TARGET_SPECS = {\n",
    "    \"n_encounters\": 0.80,\n",
    "    \"n_procedures\": 0.80,\n",
    "    \"overall_stay_days_mean\": 0.80,\n",
    "    \"overall_stay_days_max\": 0.80,\n",
    "    \"total_transfers\": 0.80,\n",
    "    \"n_distinct_event_types\": 0.80,\n",
    "    \"rx_orders_per_encounter_avg\": 0.80,\n",
    "    \"rx_admin_per_encounter_avg\": 0.80,\n",
    "}\n",
    "TARGETS = [c for c in TARGET_SPECS.keys() if c in df.columns]  # keep existing\n",
    "print(\"Targets to evaluate:\", TARGETS)  # log\n",
    "\n",
    "# ---------- Entry-time feature policy ----------\n",
    "ENTRY_MODE = \"strict\"  # choose from {\"strict\",\"liberal\"}  # strict uses fewer, safer features at intake\n",
    "\n",
    "# words that likely indicate post-visit outcomes or heavy utilization signals; drop in strict mode        \n",
    "STRICT_DROP_PATTERNS = [\n",
    "    r\"\\bstay\\b\", r\"length\", r\"hours\", r\"encounter\", r\"procedure\", r\"transfer\", r\"rx_\", r\"admin\",\n",
    "    r\"util\", r\"admit\", r\"discharge\", r\"icu\", r\"vent\", r\"mort\", r\"readmit\"\n",
    "]\n",
    "# whitelist that is safe at entry (demographics / chronic dx buckets etc.)                                 \n",
    "STRICT_KEEP_HINTS = [\n",
    "    \"age\", \"sex\", \"gender\", \"race\", \"ethnic\", \"zip\", \"payer\", \"comorb\", \"chronic\", \"history\",\n",
    "    \"icd_prefix_\", \"has_\", \"smoker\", \"diabetes\", \"hypertension\", \"copd\", \"asthma\"\n",
    "]\n",
    "\n",
    "def select_entry_features(df_cols, mode=\"strict\"):\n",
    "    cols = list(df_cols)  # all columns\n",
    "    # strip obvious non-features                                                                                       \n",
    "    drop_hard = {\"patient_id\", \"PATIENT_BIRTH_YEAR\"}  # ids/raw\n",
    "    cols = [c for c in cols if c not in drop_hard]  # drop ids\n",
    "    # split dtypes later; here just names                                                                               \n",
    "    if mode == \"strict\":\n",
    "        # keep if any keep-hint matches or it's categorical-like object                                                 \n",
    "        keep_cols = []\n",
    "        for c in cols:\n",
    "            name = c.lower()\n",
    "            if df[c].dtype == \"object\":  # categorical at intake is OK                                                 \n",
    "                keep_cols.append(c)\n",
    "            elif any(k in name for k in STRICT_KEEP_HINTS):\n",
    "                keep_cols.append(c)\n",
    "        # remove any that match drop patterns                                                                           \n",
    "        pat = re.compile(\"|\".join(STRICT_DROP_PATTERNS), flags=re.I)\n",
    "        keep_cols = [c for c in keep_cols if pat.search(c) is None]\n",
    "        return keep_cols\n",
    "    else:\n",
    "        # liberal: use everything except the obviously post-outcome signals; still remove drop patterns                 \n",
    "        pat = re.compile(\"|\".join(STRICT_DROP_PATTERNS), flags=re.I)\n",
    "        return [c for c in cols if pat.search(c) is None]\n",
    "\n",
    "ENTRY_FEATURES_ALL = select_entry_features(df.columns, ENTRY_MODE)  # base feature list\n",
    "# We will further remove the current target and any columns that leak about that target (same-root names).             \n",
    "\n",
    "# ---------- Helper: probabilistic OneHotEncoder signature compatibility ----------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_kwargs = {\"handle_unknown\":\"ignore\"}  # base kwargs\n",
    "if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
    "    onehot_kwargs[\"sparse_output\"] = True  # new api\n",
    "else:\n",
    "    onehot_kwargs[\"sparse\"] = True  # old api\n",
    "\n",
    "# ---------- Models ----------\n",
    "def build_models():\n",
    "    models = {}\n",
    "    # Logistic Regression                                                                                           \n",
    "    models[\"LogReg\"] = LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\", class_weight=\"balanced\",\n",
    "        max_iter=5000, n_jobs=-1, random_state=42\n",
    "    )\n",
    "    # Random Forest                                                                                                  \n",
    "    models[\"RandForest\"] = RandomForestClassifier(\n",
    "        n_estimators=400, min_samples_leaf=2, class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    # Extra Trees                                                                                                     \n",
    "    models[\"ExtraTrees\"] = ExtraTreesClassifier(\n",
    "        n_estimators=600, min_samples_leaf=2, class_weight=\"balanced\",\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    # Linear SVM with probability calibration                                                                         \n",
    "    base_svm = LinearSVC(dual=False, class_weight=\"balanced\", random_state=42)\n",
    "    models[\"LinearSVM_cal\"] = CalibratedClassifierCV(base_svm, method=\"sigmoid\", cv=3)\n",
    "    # Optional LightGBM                                                                                              \n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        models[\"LightGBM\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=800, learning_rate=0.05, num_leaves=64, min_child_samples=20,\n",
    "            subsample=0.8, colsample_bytree=0.8, objective=\"binary\", random_state=42, n_jobs=-1\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    return models\n",
    "\n",
    "# ---------- Build preprocess pipeline ----------\n",
    "def make_preprocess(num_cols, cat_cols):\n",
    "    num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                       (\"scaler\", StandardScaler(with_mean=False))])  # keep sparse compatibility               \n",
    "    cat_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                       (\"oh\", OneHotEncoder(**onehot_kwargs))])  # OHE                                           \n",
    "    return ColumnTransformer([(\"num\", num_tf, num_cols), (\"cat\", cat_tf, cat_cols)])\n",
    "\n",
    "# ---------- Labeling helper ----------\n",
    "def make_binary_label(series, q=0.80):\n",
    "    # Use integer ceil for count-like columns; else raw quantile threshold                                          \n",
    "    thr = series.quantile(q)\n",
    "    if pd.api.types.is_integer_dtype(series.dtype) or series.name.startswith(\"n_\"):\n",
    "        thr_int = int(np.ceil(thr))\n",
    "        y = (series >= thr_int).astype(int)\n",
    "        info = dict(threshold=thr, threshold_used=thr_int, rule=\">= ceil(p{})\".format(int(q*100)))\n",
    "    else:\n",
    "        y = (series >= thr).astype(int)\n",
    "        info = dict(threshold=thr, threshold_used=float(thr), rule=\">= p{}\".format(int(q*100)))\n",
    "    return y, info\n",
    "\n",
    "# ---------- Leakage scrub for a given target ----------\n",
    "def scrub_leakage(base_features, target_name):\n",
    "    # remove target itself and any column whose name contains the root token(s) of target                             \n",
    "    toks = re.split(r\"[_:]+\", target_name.lower())\n",
    "    toks = [t for t in toks if len(t) >= 3]  # short tokens are noisy                                                \n",
    "    def leak(c):\n",
    "        lc = c.lower()\n",
    "        if c == target_name: return True\n",
    "        return any(t in lc for t in toks)\n",
    "    feats = [c for c in base_features if not leak(c)]\n",
    "    return feats\n",
    "\n",
    "# ---------- Evaluation per target ----------\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV\n",
    "all_rows = []  # accumulate rows for mega table\n",
    "\n",
    "for tgt in TARGETS:\n",
    "    # Build label                                                                                                   \n",
    "    y, lab_info = make_binary_label(df[tgt], q=TARGET_SPECS[tgt])\n",
    "    prevalence = float(y.mean())\n",
    "    # Choose entry-safe features and scrub leakage                                                                    \n",
    "    feats0 = [c for c in ENTRY_FEATURES_ALL if c != tgt and c in df.columns]  # base entry set\n",
    "    feats  = scrub_leakage(feats0, tgt)  # remove lookalikes\n",
    "    # Split dtypes                                                                                                   \n",
    "    cat_cols = [c for c in feats if df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in feats if c not in cat_cols]\n",
    "    # Save feature list                                                                                               \n",
    "    pd.Series(feats).to_csv(os.path.join(FLIST_DIR, f\"entry_features_{ENTRY_MODE}_{tgt}.csv\"),\n",
    "                            index=False, header=False)\n",
    "    # Preprocess                                                                                                      \n",
    "    preprocess = make_preprocess(num_cols, cat_cols)\n",
    "    # Models                                                                                                          \n",
    "    models = build_models()\n",
    "\n",
    "    print(f\"\\n=== Target: {tgt} | prev={prevalence:.3%} | rule {lab_info['rule']} with used threshold={lab_info['threshold_used']} ===\")\n",
    "    # Evaluate each model by CV                                                                                       \n",
    "    for mname, base_est in models.items():\n",
    "        pipe = Pipeline([(\"prep\", preprocess), (\"clf\", base_est)])  # full pipeline\n",
    "        aucs, auprcs = [], []\n",
    "        # CV loop                                                                                                     \n",
    "        for tr, te in CV.split(df[feats], y):\n",
    "            Xtr, Xte = df.iloc[tr][feats], df.iloc[te][feats]\n",
    "            ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            # probabilities or decision\n",
    "            clf = pipe.named_steps[\"clf\"]\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                prob = pipe.predict_proba(Xte)[:, 1]\n",
    "            else:\n",
    "                # Calibrated SVM exposes predict_proba; bare LinearSVC won't reach here                               \n",
    "                dec = pipe.decision_function(Xte)\n",
    "                # min-max squash to [0,1] as fallback                                                                 \n",
    "                m, M = dec.min(), dec.max()\n",
    "                prob = (dec - m) / (M - m + 1e-9)\n",
    "            aucs.append(roc_auc_score(yte, prob))\n",
    "            auprcs.append(average_precision_score(yte, prob))\n",
    "\n",
    "        row = dict(target=tgt, prevalence=prevalence, threshold_used=lab_info[\"threshold_used\"],\n",
    "                   model=mname, AUROC=np.mean(aucs), AUROC_std=np.std(aucs),\n",
    "                   AUPRC=np.mean(auprcs), AUPRC_std=np.std(auprcs),\n",
    "                   n_features=len(feats), entry_mode=ENTRY_MODE)\n",
    "        all_rows.append(row)\n",
    "        print(f\"{mname:14s} | AUROC {np.mean(aucs):.3f}±{np.std(aucs):.3f} | AUPRC {np.mean(auprcs):.3f}±{np.std(auprcs):.3f} | feats={len(feats)}\")\n",
    "\n",
    "    # For the best model by AUROC, make OOF-ish plots (train on 4 folds, test on 1, stitched)                         \n",
    "    best = max([r for r in all_rows if r[\"target\"]==tgt], key=lambda x: x[\"AUROC\"])\n",
    "    best_name = best[\"model\"]\n",
    "    best_est = build_models()[best_name]\n",
    "    pipe_best = Pipeline([(\"prep\", preprocess), (\"clf\", best_est)])\n",
    "    # Accumulate OOF predictions                                                                                      \n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    for tr, te in CV.split(df[feats], y):\n",
    "        pipe_best.fit(df.iloc[tr][feats], y.iloc[tr])\n",
    "        clf = pipe_best.named_steps[\"clf\"]\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            oof[te] = pipe_best.predict_proba(df.iloc[te][feats])[:,1]\n",
    "        else:\n",
    "            dec = pipe_best.decision_function(df.iloc[te][feats])\n",
    "            m, M = dec.min(), dec.max()\n",
    "            oof[te] = (dec - m) / (M - m + 1e-9)\n",
    "    # Plot ROC/PR                                                                                                      \n",
    "    fpr, tpr, _ = roc_curve(y, oof)\n",
    "    prec, rec, _ = precision_recall_curve(y, oof)\n",
    "    auroc = roc_auc_score(y, oof)\n",
    "    auprc = average_precision_score(y, oof)\n",
    "\n",
    "    plt.figure(figsize=(5.2,4.4))\n",
    "    plt.plot(fpr, tpr, label=f\"{best_name} (AUROC={auroc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\", lw=1)\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {tgt} ({ENTRY_MODE})\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    roc_path = os.path.join(FIG_DIR, f\"roc_{tgt}_{best_name}_{ENTRY_MODE}.png\")\n",
    "    plt.tight_layout(); plt.savefig(roc_path, dpi=150); plt.close()\n",
    "    print(\"Saved:\", roc_path)\n",
    "\n",
    "    plt.figure(figsize=(5.2,4.4))\n",
    "    plt.plot(rec, prec, label=f\"{best_name} (AUPRC={auprc:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {tgt} ({ENTRY_MODE})\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    pr_path = os.path.join(FIG_DIR, f\"pr_{tgt}_{best_name}_{ENTRY_MODE}.png\")\n",
    "    plt.tight_layout(); plt.savefig(pr_path, dpi=150); plt.close()\n",
    "    print(\"Saved:\", pr_path)\n",
    "\n",
    "# ---------- Save mega-table ----------\n",
    "mega = pd.DataFrame(all_rows).sort_values([\"target\",\"AUROC\"], ascending=[True,False])\n",
    "mega.to_csv(os.path.join(TAB_DIR, f\"mega_auc_table_{ENTRY_MODE}.csv\"), index=False)\n",
    "print(\"\\nWrote mega table:\", os.path.join(TAB_DIR, f\"mega_auc_table_{ENTRY_MODE}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f542927-b860-4227-84c2-38670ffbdd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
